{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf60433",
   "metadata": {},
   "source": [
    "## Problem 1:\n",
    "\n",
    "##### Bayes theorem shows us how to turn $P(D|H)$ to $P(H|E)$, with $D = \\text{Data / Evidence}$ and $H = \\text{Hypothesis}$. But what does that really mean? Imagine you have to explain this to someone who doesn't understand machine learning or probability at all. How would you do it in a paragraph or two without using any jargons? Use an example from real life to ground the explanation.\n",
    "\n",
    "***\n",
    "\n",
    "_With these types of problems, they often involve a cause(s) and effect(s), one of which is known, and the other is unknown. This is what Bayes theorem helps with. Bayes theorem considers what the likeliness of a certain effect would be given some cause. In other words, it helps us determine the likeliness that a hypothesis (effect) occurs after observing some data/evidence (cause). For example, if you had two types of dice, one 4 sided and the other 6 sided, and we are told which dice was chosen at random, we could find the likeliness of a certain number being rolled. In this case, the effect/hypothesis (the number rolled) is unknown while the cause/data/evidence (which dice is chosen) is known._\n",
    "\n",
    "_However, there are also instances in which the objective is the reverse, that is, you are trying to find the likeliness of some cause (data/evidence) given a certain effect (hypothesis). We can consider a similar example with the dice, in which we see what number is already rolled, though need to determine the likeliness of which dice was chosen. The cause and effect are the same as in the previous case, though this time the knowns and unknowns are swapped, that is, the effect/hypothesis (the number rolled) is known and the cause/data/evidence (which dice is chosen) is unknown._\n",
    "\n",
    "_From this, we can see that once the knowns and unknowns are identified, Bayes theorem allows us to find the likeliness of the unknown, whether that be the cause (hypothesis) or the effect (data/evidence)._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713fd5a2",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "***\n",
    "\n",
    "## Problem 2:\n",
    "\n",
    "##### Download the  YouTube spam collection dataset.\n",
    "\n",
    "##### This is a public set of comments collected for spam research. It has five datasets composed by 1,956 real messages extracted from five videos. These 5 videos are popular pop songs that were among the 10 most viewed on the collection period. All the five dataset has the following attributes:\n",
    "\n",
    "##### `COMMENT_ID`: Unique id representing the comment\n",
    "##### `AUTHOR`: Author id,\n",
    "##### `DATE`: Date the comment is posted,\n",
    "##### `CONTENT`: The comment,\n",
    "##### `TAG`: For spam 1, otherwise 0\n",
    "\n",
    "##### For this exercise use any 4 of these 5 datasets to build a spam filter with Naive Bayes approach and use that filter to check the accuracy on the remaining dataset. Make sure to report the details of your training and the model.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4721fd2",
   "metadata": {},
   "source": [
    "_First we need to import any packages including `pandas`, `GaussianNB`, and `CountVectorizer`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac7c1ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0841df",
   "metadata": {},
   "source": [
    "_Now we can load in all of our datasets. The last one (`Youtube05-Shakira`) will act as our test data so we can assign it as such._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ee9a297",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_psy = pd.read_csv(\"YouTube-Spam-Collection-v1\\Youtube01-Psy.csv\")\n",
    "df_kp = pd.read_csv(\"YouTube-Spam-Collection-v1\\Youtube02-KatyPerry.csv\")\n",
    "df_lmfao = pd.read_csv(\"YouTube-Spam-Collection-v1\\Youtube03-LMFAO.csv\")\n",
    "df_eminem = pd.read_csv(\"YouTube-Spam-Collection-v1\\Youtube04-Eminem.csv\")\n",
    "\n",
    "df_test = pd.read_csv(\"YouTube-Spam-Collection-v1\\Youtube05-Shakira.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2788f5",
   "metadata": {},
   "source": [
    "_Since we will be using the first four datasets for training, we can combine them using `concat()` into a single dataframe._\n",
    "\n",
    "> _Note: this source was used to understand the `concat()` function: https://pandas.pydata.org/docs/reference/api/pandas.concat.html_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0da3c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_psy, df_kp, df_lmfao, df_eminem])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed777d6",
   "metadata": {},
   "source": [
    "_Then, using this combined training data, we can assign `CONTENT` as our training features and `CLASS` as our training label._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5f6f10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = df_train['CONTENT']\n",
    "train_label = df_train['CLASS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bf1adc",
   "metadata": {},
   "source": [
    "_Before continuing, we can also check the number of both spam (1) and non-spam (0) instances in our training data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a7e3435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    831\n",
       "0    755\n",
       "Name: CLASS, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b116131",
   "metadata": {},
   "source": [
    "_From this we can see that about 52.4% of the comments are spam while about 47.6% of the comments are not spam, meaning that there is a fairly even distribution between the two and our future model can be relatively equally trained on both types._\n",
    "\n",
    "_Then we can use `CountVectorizer()` to convert the training comments into a matrix of token counts that track how many times words appear in the messages. We can also set the messages to be converted to lowercase before tokenizing._\n",
    "\n",
    "> _Note: this source was used to understand `CountVectorizer()`: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eb366fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(lowercase = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efba64e4",
   "metadata": {},
   "source": [
    "_Now we can use `vectorizer` to learn the vocabulary dictionary of the messages and return a matrix of the word counts._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7585ab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_train_features = vectorizer.fit_transform(train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8bda51",
   "metadata": {},
   "source": [
    "_Next we can load in the Gaussian Naive Bayes model._\n",
    "\n",
    "> _Note: this source was used to understand the Gaussian Naive Bayes model: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d3d2cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_model = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d910c5ca",
   "metadata": {},
   "source": [
    "_Using the Gaussian Naive Bayes model, we can fit it to our vectorized training features (`vectorize_train_features`) and training label (`train_label`)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c3b2bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_fitted_model = NB_model.fit(vectorize_train_features.toarray(), train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4813fd8",
   "metadata": {},
   "source": [
    "_Now we can measure the training accuracy of our model._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6439c670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9899117276166457"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_fitted_model.score(vectorize_train_features.toarray(), train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06560822",
   "metadata": {},
   "source": [
    "_From this we see that our Gaussian Naive Bayes model is 98.99% accurate on our training data, which is very good._\n",
    "\n",
    "_Then, to test our model we first need to assign our test features and label._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "594597ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = df_test['CONTENT']\n",
    "test_label = df_test['CLASS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a867d7",
   "metadata": {},
   "source": [
    "_Now we can transform the test features to a matrix of word occurances so that we can run our model on the test data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "190bfa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_test_features = vectorizer.transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a76504",
   "metadata": {},
   "source": [
    "_Then we can measure the accuracy of our model on the test data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acc73fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8918918918918919"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_fitted_model.score(vectorize_test_features.toarray(), test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7563a7",
   "metadata": {},
   "source": [
    "_While the test accuracy declined by about 10% from our training data, a 89.19% accuracy is still very high._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d346b1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2940b2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086428ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d72d01f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78e1dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119a2d17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19cc2c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "fc0413b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [   0    2    3 ... 1582 1583 1584] TEST: [   1    7    9   22   36   49   51   59   64   69   73   80   81   93\n",
      "   99  106  113  115  117  119  122  126  128  135  144  168  169  174\n",
      "  176  178  182  183  188  189  199  205  211  213  218  220  224  231\n",
      "  234  237  239  242  249  259  270  271  276  281  287  288  293  309\n",
      "  312  313  329  333  335  343  346  351  359  360  362  363  364  366\n",
      "  367  370  379  383  393  402  403  405  406  410  419  423  427  432\n",
      "  434  437  446  449  450  463  467  468  477  496  497  505  507  509\n",
      "  517  518  520  525  536  541  549  557  560  561  568  569  570  588\n",
      "  592  593  601  602  604  616  626  628  636  650  651  655  656  670\n",
      "  673  682  688  690  697  698  706  716  717  719  730  731  736  737\n",
      "  746  750  752  756  757  759  762  782  788  790  792  797  807  811\n",
      "  821  832  839  840  851  852  854  866  873  875  876  877  879  882\n",
      "  884  889  891  898  900  905  909  911  912  917  922  923  930  931\n",
      "  941  942  949  950  956  957  961  963  966  972  977  981  982  992\n",
      "  999 1000 1009 1016 1023 1026 1028 1029 1040 1047 1056 1059 1064 1068\n",
      " 1073 1075 1079 1089 1096 1107 1113 1125 1127 1128 1139 1142 1145 1146\n",
      " 1156 1157 1161 1163 1166 1174 1180 1184 1190 1193 1194 1200 1207 1212\n",
      " 1218 1221 1228 1231 1247 1251 1259 1260 1263 1266 1267 1268 1270 1276\n",
      " 1279 1281 1289 1294 1296 1297 1300 1301 1304 1308 1326 1329 1335 1336\n",
      " 1337 1339 1347 1355 1356 1365 1373 1374 1376 1380 1390 1398 1402 1404\n",
      " 1409 1412 1413 1419 1420 1424 1427 1440 1446 1447 1455 1459 1474 1476\n",
      " 1482 1486 1490 1492 1495 1505 1506 1508 1512 1513 1524 1525 1534 1541\n",
      " 1543 1545 1546 1548 1559 1563 1568 1575 1580 1585]\n",
      "TRAIN: [   0    1    2 ... 1582 1584 1585] TEST: [   5    6   14   15   18   29   30   32   39   41   43   50   52   54\n",
      "   60   70   72   74   76   77   78   87   90   96   98  116  124  127\n",
      "  132  133  134  148  153  160  161  163  164  173  190  191  197  200\n",
      "  206  212  219  223  226  230  233  238  243  245  252  254  268  280\n",
      "  285  298  299  301  302  303  305  311  315  322  327  331  347  353\n",
      "  357  374  377  380  382  395  397  399  404  408  413  420  430  435\n",
      "  439  440  454  462  464  466  471  479  480  481  484  487  489  491\n",
      "  498  500  503  506  508  510  514  531  537  540  546  547  548  550\n",
      "  552  555  559  562  567  581  583  585  591  595  596  609  611  612\n",
      "  614  620  621  622  627  630  638  643  644  647  658  663  665  671\n",
      "  672  676  677  683  684  686  687  691  699  700  708  721  724  725\n",
      "  728  739  740  741  743  744  761  765  770  775  778  785  787  791\n",
      "  794  795  798  802  812  816  817  827  828  833  834  836  841  842\n",
      "  846  863  868  888  892  893  906  907  914  924  928  932  933  935\n",
      "  936  948  952  958  964  965  973  978  983  986  987  988  991  995\n",
      "  996 1013 1015 1017 1020 1038 1041 1042 1049 1057 1072 1077 1084 1087\n",
      " 1097 1100 1103 1111 1112 1120 1121 1129 1131 1138 1140 1143 1144 1148\n",
      " 1159 1175 1177 1186 1205 1208 1215 1224 1225 1230 1232 1234 1237 1238\n",
      " 1249 1256 1261 1262 1264 1278 1291 1293 1305 1310 1313 1314 1316 1319\n",
      " 1321 1340 1341 1345 1352 1353 1354 1359 1361 1362 1370 1384 1385 1391\n",
      " 1392 1395 1400 1403 1405 1406 1408 1410 1416 1422 1425 1430 1431 1432\n",
      " 1434 1438 1444 1449 1460 1470 1483 1487 1488 1497 1499 1509 1517 1518\n",
      " 1519 1523 1527 1537 1554 1560 1564 1565 1583]\n",
      "TRAIN: [   1    2    5 ... 1583 1584 1585] TEST: [   0    3    4   12   19   21   33   35   37   38   48   55   58   62\n",
      "   66   75   83   85   91   94   95  107  112  138  139  140  141  142\n",
      "  150  151  154  158  162  167  177  186  192  194  203  207  222  227\n",
      "  229  235  240  253  256  263  264  266  272  273  274  277  279  289\n",
      "  292  295  296  300  306  307  316  323  324  330  332  341  342  348\n",
      "  352  355  361  371  373  394  396  400  401  409  415  416  418  425\n",
      "  433  436  438  441  442  444  447  453  456  459  473  474  483  494\n",
      "  512  515  516  519  521  528  530  533  535  538  539  544  554  563\n",
      "  565  566  579  582  584  586  587  594  597  598  606  607  610  623\n",
      "  629  641  645  646  649  678  681  685  695  696  704  705  709  711\n",
      "  718  732  747  748  749  753  764  768  771  773  776  781  783  801\n",
      "  813  814  819  824  825  829  831  835  837  844  847  848  859  860\n",
      "  865  869  874  880  885  886  890  894  899  901  902  910  913  920\n",
      "  921  929  940  943  947  955  968  969  971  980  990  994 1001 1002\n",
      " 1003 1005 1008 1012 1018 1021 1032 1037 1044 1046 1050 1051 1060 1065\n",
      " 1074 1080 1081 1083 1091 1093 1094 1104 1105 1110 1119 1141 1149 1153\n",
      " 1154 1158 1162 1165 1169 1171 1173 1179 1181 1185 1187 1188 1197 1201\n",
      " 1202 1209 1211 1217 1220 1223 1236 1241 1243 1244 1245 1250 1252 1253\n",
      " 1254 1265 1275 1282 1292 1295 1317 1324 1325 1327 1330 1332 1333 1343\n",
      " 1349 1351 1366 1372 1381 1382 1383 1386 1388 1394 1399 1401 1415 1421\n",
      " 1423 1439 1441 1442 1443 1445 1448 1450 1454 1456 1457 1461 1463 1468\n",
      " 1469 1471 1480 1484 1507 1510 1511 1514 1526 1531 1538 1539 1544 1547\n",
      " 1551 1556 1557 1558 1567 1570 1574 1579 1582]\n",
      "TRAIN: [   0    1    2 ... 1582 1583 1585] TEST: [   8   10   13   23   26   27   34   40   44   45   46   47   53   56\n",
      "   61   63   67   79   86   88  102  104  111  114  120  121  125  130\n",
      "  147  149  156  159  165  166  172  179  181  184  185  193  198  208\n",
      "  210  217  221  228  232  236  246  247  248  250  255  260  265  267\n",
      "  269  275  278  291  310  314  318  325  328  334  336  337  340  344\n",
      "  349  350  354  358  365  368  372  385  388  389  391  392  407  411\n",
      "  414  422  424  426  428  431  443  445  448  451  455  458  465  472\n",
      "  476  485  486  488  495  499  501  502  523  532  534  545  553  556\n",
      "  558  564  571  572  573  578  580  589  590  599  603  608  613  618\n",
      "  624  625  631  633  634  635  637  640  661  662  666  675  689  692\n",
      "  694  702  703  707  727  734  735  738  751  758  763  766  769  772\n",
      "  774  777  779  780  786  793  796  803  808  809  810  818  830  838\n",
      "  843  845  853  861  862  864  870  871  878  896  904  908  918  927\n",
      "  934  937  938  939  945  953  959  960  962  967  975  979  984  985\n",
      "  989 1006 1010 1011 1014 1025 1027 1030 1031 1033 1034 1035 1043 1045\n",
      " 1048 1055 1061 1062 1063 1070 1078 1085 1086 1088 1090 1095 1098 1106\n",
      " 1109 1114 1116 1117 1118 1122 1123 1130 1133 1147 1150 1151 1152 1155\n",
      " 1164 1168 1176 1178 1183 1189 1191 1192 1196 1198 1199 1210 1214 1222\n",
      " 1229 1235 1239 1271 1283 1285 1290 1298 1306 1307 1309 1312 1322 1328\n",
      " 1334 1346 1350 1357 1358 1364 1367 1368 1375 1377 1378 1389 1393 1396\n",
      " 1407 1414 1417 1418 1426 1428 1429 1435 1437 1458 1464 1465 1466 1478\n",
      " 1479 1485 1491 1493 1496 1498 1500 1501 1504 1515 1528 1529 1530 1533\n",
      " 1536 1552 1553 1555 1569 1572 1573 1577 1584]\n",
      "TRAIN: [   0    1    3 ... 1583 1584 1585] TEST: [   2   11   16   17   20   24   25   28   31   42   57   65   68   71\n",
      "   82   84   89   92   97  100  101  103  105  108  109  110  118  123\n",
      "  129  131  136  137  143  145  146  152  155  157  170  171  175  180\n",
      "  187  195  196  201  202  204  209  214  215  216  225  241  244  251\n",
      "  257  258  261  262  282  283  284  286  290  294  297  304  308  317\n",
      "  319  320  321  326  338  339  345  356  369  375  376  378  381  384\n",
      "  386  387  390  398  412  417  421  429  452  457  460  461  469  470\n",
      "  475  478  482  490  492  493  504  511  513  522  524  526  527  529\n",
      "  542  543  551  574  575  576  577  600  605  615  617  619  632  639\n",
      "  642  648  652  653  654  657  659  660  664  667  668  669  674  679\n",
      "  680  693  701  710  712  713  714  715  720  722  723  726  729  733\n",
      "  742  745  754  755  760  767  784  789  799  800  804  805  806  815\n",
      "  820  822  823  826  849  850  855  856  857  858  867  872  881  883\n",
      "  887  895  897  903  915  916  919  925  926  944  946  951  954  970\n",
      "  974  976  993  997  998 1004 1007 1019 1022 1024 1036 1039 1052 1053\n",
      " 1054 1058 1066 1067 1069 1071 1076 1082 1092 1099 1101 1102 1108 1115\n",
      " 1124 1126 1132 1134 1135 1136 1137 1160 1167 1170 1172 1182 1195 1203\n",
      " 1204 1206 1213 1216 1219 1226 1227 1233 1240 1242 1246 1248 1255 1257\n",
      " 1258 1269 1272 1273 1274 1277 1280 1284 1286 1287 1288 1299 1302 1303\n",
      " 1311 1315 1318 1320 1323 1331 1338 1342 1344 1348 1360 1363 1369 1371\n",
      " 1379 1387 1397 1411 1433 1436 1451 1452 1453 1462 1467 1472 1473 1475\n",
      " 1477 1481 1489 1494 1502 1503 1516 1520 1521 1522 1532 1535 1540 1542\n",
      " 1549 1550 1561 1562 1566 1571 1576 1578 1581]\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "for train_index, test_index in skf.split(features, label):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "    y_train, y_test = label.iloc[train_index], label.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2776e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c539980f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6c6ec926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [huh,, anyway, check, out, this, you[tube], ch...\n",
       "1      [hey, guys, check, out, my, new, channel, and,...\n",
       "2        [just, for, test, i, have, to, say, murdev.com]\n",
       "3      [me, shaking, my, sexy, ass, on, my, channel, ...\n",
       "4            [watch?v=vtarggvgtwq, check, this, out, .﻿]\n",
       "                             ...                        \n",
       "443     [subscribe, to, my, channel, x, please!., spare]\n",
       "444    [check, out, my, videos, guy!, :), hope, you, ...\n",
       "445    [3, yrs, ago, i, had, a, health, scare, but, t...\n",
       "446    [rihanna, looks, so, beautiful, with, red, hai...\n",
       "447    [857.482.940, views, awesome, !!!!!!!!!!!!!!!!...\n",
       "Name: CONTENT, Length: 1586, dtype: object"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_split = features.str.split()\n",
    "features_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c1c6b2",
   "metadata": {},
   "source": [
    "Note: the following resource was used to resolve an error:\n",
    "> https://stackoverflow.com/questions/51852551/key-error-not-in-index-while-cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "487984be",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19924/1133362400.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlowercase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#vectorizer.get_feature_names_out()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1200\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1202\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1203\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mngrams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(lowercase = False)\n",
    "X = vectorizer.fit_transform(X_train)\n",
    "#vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "\n",
    "#vectorizer = CountVectorizer().fit(X_train)\n",
    "#X_train_vectorized = vectorizer.transform(X_train)\n",
    "#X_train_vectorized.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d84b77b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a6e3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6bca4697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build \"dictionary\" of all words in the comments\n",
    "dict = []\n",
    "\n",
    "for comment in X_train:\n",
    "    for word in comment:\n",
    "        if (word not in dict):\n",
    "            dict.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53781669",
   "metadata": {},
   "source": [
    "Note: the following two resources were used to understand how to structure this as well as how to find if an element in not in a list:\n",
    "> https://www.kdnuggets.com/2020/07/spam-filter-python-naive-bayes-scratch.html\n",
    "\n",
    "> https://stackoverflow.com/questions/10406130/check-if-something-is-not-in-a-list-in-python/10406143"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "536c8bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_spam_count = {}\n",
    "\n",
    "for word in dict:\n",
    "    word_appearance = 0\n",
    "    for comment in X_train:\n",
    "        if word in comment:\n",
    "            word_appearance = word_appearance + 1\n",
    "            \n",
    "#    total_spam = len(train_spam)\n",
    "#    spamicity = (emails_with_w+1)/(total_spam+2)\n",
    "#    dict_spamicity[w.lower()] = spamicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d748162c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_spam_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c523148",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ec9e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41537535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f34cd19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2517067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data = [0] * len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8639a6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lefta\\AppData\\Local\\Temp/ipykernel_19924/2795059388.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_dict[word] = base_data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>huh,</th>\n",
       "      <th>anyway</th>\n",
       "      <th>check</th>\n",
       "      <th>out</th>\n",
       "      <th>this</th>\n",
       "      <th>you[tube]</th>\n",
       "      <th>channel:</th>\n",
       "      <th>kobyoshi02</th>\n",
       "      <th>hey</th>\n",
       "      <th>guys</th>\n",
       "      <th>...</th>\n",
       "      <th>things?</th>\n",
       "      <th>motivate</th>\n",
       "      <th>they’ve</th>\n",
       "      <th>you’re</th>\n",
       "      <th>far!</th>\n",
       "      <th>1000</th>\n",
       "      <th>started!</th>\n",
       "      <th>red</th>\n",
       "      <th>857.482.940</th>\n",
       "      <th>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!﻿</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1269 rows × 4476 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      huh,  anyway  check  out  this  you[tube]  channel:  kobyoshi02  hey  \\\n",
       "0        0       0      0    0     0          0         0           0    0   \n",
       "1        0       0      0    0     0          0         0           0    0   \n",
       "2        0       0      0    0     0          0         0           0    0   \n",
       "3        0       0      0    0     0          0         0           0    0   \n",
       "4        0       0      0    0     0          0         0           0    0   \n",
       "...    ...     ...    ...  ...   ...        ...       ...         ...  ...   \n",
       "1264     0       0      0    0     0          0         0           0    0   \n",
       "1265     0       0      0    0     0          0         0           0    0   \n",
       "1266     0       0      0    0     0          0         0           0    0   \n",
       "1267     0       0      0    0     0          0         0           0    0   \n",
       "1268     0       0      0    0     0          0         0           0    0   \n",
       "\n",
       "      guys  ...  things?  motivate  they’ve  you’re  far!  1000  started!  \\\n",
       "0        0  ...        0         0        0       0     0     0         0   \n",
       "1        0  ...        0         0        0       0     0     0         0   \n",
       "2        0  ...        0         0        0       0     0     0         0   \n",
       "3        0  ...        0         0        0       0     0     0         0   \n",
       "4        0  ...        0         0        0       0     0     0         0   \n",
       "...    ...  ...      ...       ...      ...     ...   ...   ...       ...   \n",
       "1264     0  ...        0         0        0       0     0     0         0   \n",
       "1265     0  ...        0         0        0       0     0     0         0   \n",
       "1266     0  ...        0         0        0       0     0     0         0   \n",
       "1267     0  ...        0         0        0       0     0     0         0   \n",
       "1268     0  ...        0         0        0       0     0     0         0   \n",
       "\n",
       "      red  857.482.940  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!﻿  \n",
       "0       0            0                                            0  \n",
       "1       0            0                                            0  \n",
       "2       0            0                                            0  \n",
       "3       0            0                                            0  \n",
       "4       0            0                                            0  \n",
       "...   ...          ...                                          ...  \n",
       "1264    0            0                                            0  \n",
       "1265    0            0                                            0  \n",
       "1266    0            0                                            0  \n",
       "1267    0            0                                            0  \n",
       "1268    0            0                                            0  \n",
       "\n",
       "[1269 rows x 4476 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict = pd.DataFrame()\n",
    "\n",
    "for word in dict:\n",
    "   df_dict[word] = base_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c3af734f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19924/1325155801.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mbase_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "#for index, row in enumerate(X_train):\n",
    "#    for word in X_train:\n",
    "#        base_data[word][index] += 1\n",
    "        \n",
    "for index, row in enumerate(X_train):\n",
    "    for word in row:\n",
    "        base_data[word][index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6512c9e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [huh,, anyway, check, out, this, you[tube], ch...\n",
       "1      [hey, guys, check, out, my, new, channel, and,...\n",
       "2        [just, for, test, i, have, to, say, murdev.com]\n",
       "3      [me, shaking, my, sexy, ass, on, my, channel, ...\n",
       "4            [watch?v=vtarggvgtwq, check, this, out, .﻿]\n",
       "                             ...                        \n",
       "441                             [best., song., ever, 🙌﻿]\n",
       "443     [subscribe, to, my, channel, x, please!., spare]\n",
       "445    [3, yrs, ago, i, had, a, health, scare, but, t...\n",
       "446    [rihanna, looks, so, beautiful, with, red, hai...\n",
       "447    [857.482.940, views, awesome, !!!!!!!!!!!!!!!!...\n",
       "Name: CONTENT, Length: 1269, dtype: object"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e762694c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0806c67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f9faed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7ca2cec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b7c15ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\", line 207, in fit\n",
      "    X, y = self._validate_data(X, y)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 433, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 871, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 673, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\", line 102, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\", line 857, in __array__\n",
      "    return np.asarray(self._values, dtype)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\", line 102, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "ValueError: could not convert string to float: 'Huh, anyway check out this you[tube] channel: kobyoshi02'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\", line 207, in fit\n",
      "    X, y = self._validate_data(X, y)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 433, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 871, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 673, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\", line 102, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\", line 857, in __array__\n",
      "    return np.asarray(self._values, dtype)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\", line 102, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "ValueError: could not convert string to float: 'Huh, anyway check out this you[tube] channel: kobyoshi02'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\", line 207, in fit\n",
      "    X, y = self._validate_data(X, y)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 433, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 871, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 673, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\", line 102, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\", line 857, in __array__\n",
      "    return np.asarray(self._values, dtype)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\", line 102, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "ValueError: could not convert string to float: \"Hey guys check out my new channel and our first vid THIS IS US THE  MONKEYS!!! I'm the monkey in the white shirt,please leave a like comment  and please subscribe!!!!\"\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\", line 207, in fit\n",
      "    X, y = self._validate_data(X, y)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 433, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 871, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 673, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\", line 102, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\", line 857, in __array__\n",
      "    return np.asarray(self._values, dtype)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\", line 102, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "ValueError: could not convert string to float: 'Huh, anyway check out this you[tube] channel: kobyoshi02'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\", line 207, in fit\n",
      "    X, y = self._validate_data(X, y)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 433, in _validate_data\n",
      "    X, y = check_X_y(X, y, **check_params)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 871, in check_X_y\n",
      "    X = check_array(X, accept_sparse=accept_sparse,\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 63, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 673, in check_array\n",
      "    array = np.asarray(array, order=order, dtype=dtype)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\", line 102, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\", line 857, in __array__\n",
      "    return np.asarray(self._values, dtype)\n",
      "  File \"C:\\Users\\lefta\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\", line 102, in asarray\n",
      "    return array(a, dtype, copy=False, order=order)\n",
      "ValueError: could not convert string to float: 'Huh, anyway check out this you[tube] channel: kobyoshi02'\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    }
   ],
   "source": [
    "cross_val = cross_validate(NB_model, features, label, cv = kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3105e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
